# Performance Quick Reference Guide

## TL;DR - Critical Findings

### âœ… What's Working Well
- **Request deduplication**: Excellent (80-90% reduction in concurrent requests)
- **Cache hit performance**: O(1), ~50ns - optimal
- **Statistics overhead**: <0.001% CPU - negligible

### âš ï¸ What Needs Fixing (Priority Order)

1. **CRITICAL: Unbounded memory growth**
   - Current: No size limits = OOM risk
   - Fix: Add LRU with maxSize=500 per module
   - Effort: 2-4 hours

2. **CRITICAL: O(n) cleanup overhead**
   - Current: Full cache scan every cleanup
   - Fix: Lazy expiration (check on access)
   - Effort: 1-2 hours

3. **IMPORTANT: Invalidation race condition**
   - Current: Stale data after cache invalidation
   - Fix: Clear pending requests on invalidation
   - Effort: 30 minutes

---

## Performance Metrics at a Glance

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Cache hit latency | ~50ns | <100ns | âœ… Excellent |
| Memory usage | Unbounded | <50MB | ğŸ”´ Critical |
| Cleanup time (1K entries) | ~100Î¼s | <10Î¼s | ğŸŸ¡ Needs optimization |
| Deduplication efficiency | 80-90% | >70% | âœ… Excellent |
| API call reduction | 30-50% | >30% | âœ… Meeting target |

---

## Implementation Priority Matrix

```
High Impact  â”‚ 1. LRU Limits    â”‚ 3. Invalidation Fix
             â”‚ (Do First)       â”‚ (Quick Win)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Low Impact   â”‚ 2. Lazy Expiry   â”‚ 4. Monitoring
             â”‚ (Do Next)        â”‚ (Future)

             Low Effort         High Effort
```

---

## Time Complexity Reference

| Operation | Current | Optimized | Frequency | Impact |
|-----------|---------|-----------|-----------|--------|
| `get()` - cache hit | O(1) | O(1) | Very High | None |
| `get()` - cache miss | O(1) | O(1) | Medium | None |
| `get()` - dedup hit | O(1) | O(1) | Low | None |
| `invalidate()` | O(n) | O(n) | Low | Low |
| `cleanup()` | O(n) | O(1) | Medium | **High** âš ï¸ |
| `getStats()` | O(1) | O(1) | Low | None |

---

## Memory Footprint Calculator

### Current Implementation (No Limits)

```
Per cache entry:
- Key: ~30 bytes (average)
- Timestamp: 8 bytes
- Data: 1KB - 100KB (variable)
- Map overhead: 40 bytes

Example scenario (1000 req/hour, 5-min TTL):
- Unique keys: ~83 (1000 Ã— 5/60)
- Avg response: 10KB
- Memory: 83 Ã— 10KB = 830KB per module
- Total (7 modules): ~5.8MB âœ… Acceptable

High-traffic scenario (10,000 req/hour, 15-min TTL):
- Unique keys: ~2,500
- Memory: 2,500 Ã— 10KB = 25MB per module
- Total: 175MB âš ï¸ Getting risky

Peak scenario (100,000 req/hour, 15-min TTL):
- Unique keys: ~25,000
- Memory: 25,000 Ã— 10KB = 250MB per module
- Total: 1.75GB ğŸ”´ CRITICAL - OOM likely
```

### Optimized Implementation (LRU max=500)

```
Max entries: 500 per module
Avg response: 10KB
Max memory: 500 Ã— 10KB = 5MB per module
Total (7 modules): 35MB âœ… Safe

Worst case (100KB responses):
Max memory: 500 Ã— 100KB = 50MB per module
Total: 350MB âœ… Still acceptable
```

---

## Quick Fix Checklist

### Fix #1: Add LRU (2-4 hours)

**File:** `/src/client/modules/base-client.ts`

```bash
# Install dependency
npm install lru-cache

# Update CacheManager class
# Replace: private cache: Map<string, CacheEntry<any>> = new Map();
# With: private cache: LRU<string, CacheEntry<any>>;
```

**Changes required:**
- [ ] Add `lru-cache` import
- [ ] Update constructor to create LRU instance
- [ ] Remove manual `cleanup()` method (LRU auto-evicts)
- [ ] Update tests to handle LRU behavior

**Test command:**
```bash
npm test -- cache-manager.test.ts
```

---

### Fix #2: Lazy Expiration (1-2 hours)

**File:** `/src/client/modules/base-client.ts`

**Changes required:**
- [ ] Add expiry check in `get()` before returning cached value
- [ ] Delete expired entries on access (O(1) instead of O(n))
- [ ] Add throttled background cleanup (every 5 minutes)
- [ ] Update tests for lazy expiration behavior

**Code pattern:**
```typescript
const cached = this.cache.get(key);
if (cached) {
  if (cached.expiresAt > Date.now()) {
    return cached.data;  // âœ… Still valid
  }
  this.cache.delete(key);  // âœ… Expired - remove lazily
}
```

---

### Fix #3: Invalidation Race (30 minutes)

**File:** `/src/client/modules/base-client.ts`

**Changes required:**
- [ ] Update `invalidate()` to clear pending requests
- [ ] Add test for invalidation during in-flight request

**Code pattern:**
```typescript
invalidate(pattern: string | RegExp): void {
  const regex = typeof pattern === 'string' ? new RegExp(pattern) : pattern;

  // Clear cache entries
  for (const key of this.cache.keys()) {
    if (regex.test(key)) this.cache.delete(key);
  }

  // âœ… Also clear pending requests
  for (const key of this.pendingRequests.keys()) {
    if (regex.test(key)) this.pendingRequests.delete(key);
  }
}
```

---

## Performance Testing Commands

### Run existing cache tests
```bash
npm test -- cache-manager.test.ts
npm test -- cache-integration.test.ts
```

### Add performance benchmarks
```bash
# Create: test/performance/cache-benchmarks.test.ts
# Run: npm test -- cache-benchmarks.test.ts
```

### Monitor cache stats in production
```typescript
// In your application code
setInterval(() => {
  const stats = client.getCacheStats();
  console.log('Cache stats:', stats);
}, 60000);  // Every minute
```

---

## Expected Performance Improvements

### Before Optimizations
```
âœ— Memory: Unbounded (OOM risk)
âœ— Cleanup: O(n) full scan
âœ— Race condition: Stale data possible
âœ“ Deduplication: 80-90% reduction
âœ“ Cache hits: ~50ns
```

### After Optimizations
```
âœ“ Memory: <50MB (bounded)
âœ“ Cleanup: O(1) lazy expiration
âœ“ Race condition: Fixed
âœ“ Deduplication: 80-90% reduction (preserved)
âœ“ Cache hits: ~60ns (minimal overhead)
```

### Estimated Gains
- **Memory safety**: 100% elimination of OOM risk
- **Cleanup efficiency**: 99% reduction in cleanup time
- **Data correctness**: Zero stale data after invalidation
- **API reduction**: 30-50% (preserved from original implementation)

---

## Monitoring Recommendations

### Metrics to Track

1. **Cache hit rate** (already tracked)
   - Target: >60% for read-heavy workloads
   - Alert if: <30% (cache not effective)

2. **Cache size** (add monitoring)
   - Target: <500 entries per module
   - Alert if: Approaching max (need larger cache)

3. **Memory usage** (add monitoring)
   - Target: <50MB total
   - Alert if: >100MB (memory leak or too many large responses)

4. **API latency** (cached vs uncached)
   - Target: <10ms cached, <500ms uncached
   - Alert if: Cached >50ms (cache overhead too high)

### Sample Monitoring Code

```typescript
// Add to BusinessMapClient
getDetailedStats() {
  const stats = this.getCacheStats();
  return {
    ...stats,
    totalMemory: this.estimateTotalMemory(),
    avgCachedLatency: this.getAvgCachedLatency(),
    avgUncachedLatency: this.getAvgUncachedLatency(),
  };
}
```

---

## Common Questions

### Q: Should I increase cache TTL to reduce API calls?

**A:** Be cautious. Longer TTL = more stale data risk.

```
Recommended TTLs:
- Static data (card types, users): 15-60 minutes
- Semi-static (workspaces): 5-15 minutes
- Dynamic (cards, boards): 1-5 minutes
```

### Q: When should I call `cleanup()`?

**A:** After implementing lazy expiration, you rarely need to call it manually.

```
Current: Manual cleanup every N minutes
Optimized: Automatic lazy cleanup on access
Occasional: Background sweep every 5+ minutes
```

### Q: How many entries should I cache?

**A:** Depends on your traffic pattern.

```
Low traffic (<100 req/hour): maxSize=100
Medium traffic (1K-10K req/hour): maxSize=500
High traffic (>10K req/hour): maxSize=1000+
```

### Q: Should I cache all endpoints?

**A:** No. Cache read-heavy, low-change endpoints.

```
âœ… Good candidates:
- getUsers() (rarely changes)
- getCardTypes() (static)
- getWorkspaces() (low change rate)

âŒ Poor candidates:
- listCards() with filters (high variability)
- Real-time data (always stale)
- Write operations (never cache)
```

---

## Links to Full Analysis

- **Full Performance Analysis:** [performance-analysis.md](./performance-analysis.md)
- **Test Results:** `/test/cache-manager.test.ts`
- **Integration Tests:** `/test/integration/cache-integration.test.ts`

---

**Last Updated:** 2025-10-29
**Analyst:** Performance Engineer
